{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41652f59-1798-4431-90dc-592dd4f64a7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load EPA GHG Major Emitters data from 2021 datasets (see https://www.epa.gov/ghgreporting/data-sets) for original sources\n",
    "\n",
    "Copyright (C) 2021 OS-Climate\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "### We have local copies rooted in the S3_BUCKET : s3://redhat-osc-physical-landing-647521352890/EPA/ghgp_data_parent_company_10_2021.xlsx and s3://redhat-osc-physical-landing-647521352890/EPA/ghgp_2020_data_summary_spreadsheets/\n",
    "\n",
    "Contributed by Michael Tiemann (Github: MichaelTiemannOSC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92377eb7-1d1b-4662-ac08-99877153832b",
   "metadata": {},
   "source": [
    "Load Credentials and Data Commons libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac18bf3b-80d7-4b25-8ae4-9273709a0789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From the AWS Account page, copy the export scripts from the appropriate role using the \"Command Line or Programmatic Access\" link\n",
    "# Paste the copied text into ~/credentials.env\n",
    "\n",
    "from dotenv import dotenv_values, load_dotenv\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "dotenv_dir = os.environ.get('CREDENTIAL_DOTENV_DIR', os.environ.get('PWD', '/opt/app-root/src'))\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / 'credentials.env'\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path,override=True)\n",
    "\n",
    "import osc_ingest_trino as osc\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc72b432-76cc-41b9-be13-f90f89f05107",
   "metadata": {},
   "source": [
    "Create an S3 resource for the bucket holding source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7527df23-a9b2-4084-93aa-26ae6f3685bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3_source = boto3.resource(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ['S3_LANDING_ENDPOINT'],\n",
    "    aws_access_key_id=os.environ['S3_LANDING_ACCESS_KEY'],\n",
    "    aws_secret_access_key=os.environ['S3_LANDING_SECRET_KEY'],\n",
    ")\n",
    "bucket = s3_source.Bucket(os.environ['S3_LANDING_BUCKET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3be756-8a5e-4b7f-97d7-724244e99975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trino\n",
    "from sqlalchemy.engine import create_engine\n",
    "\n",
    "env_var_prefix = 'TRINO'\n",
    "\n",
    "sqlstring = 'trino://{user}@{host}:{port}/'.format(\n",
    "    user = os.environ[f'{env_var_prefix}_USER'],\n",
    "    host = os.environ[f'{env_var_prefix}_HOST'],\n",
    "    port = os.environ[f'{env_var_prefix}_PORT']\n",
    ")\n",
    "\n",
    "ingest_catalog = 'osc_datacommons_dev'\n",
    "ingest_schema = 'sandbox'\n",
    "\n",
    "sqlargs = {\n",
    "    'auth': trino.auth.JWTAuthentication(os.environ[f'{env_var_prefix}_PASSWD']),\n",
    "    'http_scheme': 'https',\n",
    "    'catalog': ingest_catalog,\n",
    "    'schema': ingest_schema,\n",
    "}\n",
    "\n",
    "engine = create_engine(sqlstring, connect_args = sqlargs)\n",
    "connection = engine.connect()\n",
    "\n",
    "# Show available schemas to ensure trino connection is set correctly\n",
    "qres = engine.execute('show schemas')\n",
    "qres.fetchall()\n",
    "\n",
    "trino_bucket = osc.attach_s3_bucket(\"S3_DEV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c467dc-abb1-4efa-aefb-f631a5774bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_meta_key_fields = 'metafields'\n",
    "custom_meta_key = 'metaset'\n",
    "\n",
    "qres = engine.execute(f'create schema if not exists {ingest_schema}')\n",
    "qres.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea005ec-33d4-4334-9c04-7713b343f0e1",
   "metadata": {},
   "source": [
    "Time for the Pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4dd114-6e6f-4a72-86c0-ea64c7ac8138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a26e42c-93bd-4310-9534-cf37e9073dc3",
   "metadata": {},
   "source": [
    "For osc_datacommons_dev, a trino pipeline is a parquet data stored in the S3_DEV_BUCKET\n",
    "It is a 5-step process to get there from a pandas dataframe"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1213d8ad-9163-4322-9248-d2d698752d68",
   "metadata": {},
   "source": [
    "def create_trino_pipeline (s3, schemaname, tablename, timestamp, df, meta_fields, meta_content):\n",
    "    global ingest_uuid\n",
    "    global custom_meta_key_fields, custom_meta_key\n",
    "    \n",
    "    # First convert dataframe to pyarrow for type conversion and basic metadata\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    # Second, since pyarrow tables are immutable, create a new table with additional combined metadata\n",
    "    if meta_fields or meta_content:\n",
    "        meta_json_fields = json.dumps(meta_fields)\n",
    "        meta_json = json.dumps(meta_content)\n",
    "        existing_meta = table.schema.metadata\n",
    "        combined_meta = {\n",
    "            custom_meta_key_fields.encode(): meta_json_fields.encode(),\n",
    "            custom_meta_key.encode(): meta_json.encode(),\n",
    "            **existing_meta\n",
    "        }\n",
    "        table = table.replace_schema_metadata(combined_meta)\n",
    "    # Third, convert table to parquet format (which cannot be written directly to s3)\n",
    "    pq.write_table(table, f'/tmp/{schemaname}.{tablename}.{ingest_uuid}.{timestamp}.parquet')\n",
    "    # df.to_parquet(f'/tmp/{schemaname}.{tablename}.{uuid}.parquet', index=False)\n",
    "    # Fourth, put the parquet-ified data into our S3 bucket for trino.  We cannot compute parquet format directly to S3 but we can copy it once computed\n",
    "    s3.upload_file(\n",
    "        Bucket=os.environ['S3_DEV_BUCKET'],\n",
    "        Key=f'trino/{schemaname}/{tablename}/{ingest_uuid}/{timestamp}/data.parquet',\n",
    "        Filename=f'/tmp/{schemaname}.{tablename}.{ingest_uuid}.{timestamp}.parquet'\n",
    "    )\n",
    "    # Finally, create the trino table backed by our parquet files enhanced by our metadata\n",
    "    cur.execute(f'drop table if exists {schemaname}.{tablename}')\n",
    "    print(f'dropping table: {tablename}')\n",
    "    cur.fetchall()\n",
    "    \n",
    "    columnschema = create_table_schema_pairs(df)\n",
    "\n",
    "    tabledef = f\"\"\"create table if not exists {schemaname}.{tablename}(\n",
    "{columnschema}\n",
    ") with (\n",
    "    format = 'parquet',\n",
    "    external_location = 's3a://{os.environ['S3_DEV_BUCKET']}/trino/{schemaname}/{tablename}/{ingest_uuid}/{timestamp}'\n",
    ")\"\"\"\n",
    "    print(tabledef)\n",
    "\n",
    "    # tables created externally may not show up immediately in cloud-beaver\n",
    "    cur.execute(tabledef)\n",
    "    cur.fetchall()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb734253-8d94-42d8-996d-cc9a27eab1ce",
   "metadata": {},
   "source": [
    "Prepare GLEIF matching data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677c9840-1f07-4d3c-ab34-ca401dc543e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load EPA GHGP data file using pandas *read_excel*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e16bbe-d602-4ad5-b9d7-a522536ca352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "bObj = bucket.Object('EPA/ghgp_data_parent_company_10_2021.xlsx')\n",
    "ghgp_bytes = io.BytesIO(bObj.get()['Body'].read())\n",
    "timestamp = bObj.last_modified.isoformat()\n",
    "\n",
    "# The source data mistakenly codes ZIP as a number, which means ZIP codes like 02134 are coded as 2134\n",
    "\n",
    "ghgp_parent = pd.read_excel(ghgp_bytes, sheet_name=None, dtype={'FRS ID (FACILITY)':'string', 'FACILITY NAICS CODE':'string', 'PARENT CO. PERCENT OWNERSHIP':'float32'}, parse_dates=['REPORTING YEAR'], date_parser=lambda x: pd.to_datetime(x, format='%Y'), engine='openpyxl')\n",
    "for year in ghgp_parent.keys():\n",
    "    df = ghgp_parent[year]\n",
    "    df.loc[df['PARENT COMPANY NAME']=='INTERNATIONAL PAPER CO', ['PARENT CO. STREET ADDRESS']] = '6400 Poplar Ave.'\n",
    "    df.loc[df['PARENT COMPANY NAME']=='Iowa Army Ammunition Plant', ['PARENT CO. STREET ADDRESS']] = '17571 DMC Highway 79'\n",
    "    df.loc[df['FACILITY NAME']=='Avon Lake Power Plant', ['FACILITY ADDRESS']] = '16157 Co Rd 22'\n",
    "    df.loc[df['FACILITY NAME']=='ENCANA OIL AND GAS USA - FORT LUPTON GAS PLANT', ['FACILITY ADDRESS']] = '33570 Lake Rd'\n",
    "    df['FACILITY ZIP'] = df['FACILITY ZIP'].astype(str).str.zfill(5)\n",
    "    df['PARENT CO. ZIP'] = df['PARENT CO. ZIP'].astype(str).str.zfill(5)\n",
    "    # print(year + ' ' + str([x for x in ghgp_parent[year]['PARENT CO. STREET ADDRESS'].to_list() if x and re.match(r'^\\d+$', str(x))]))\n",
    "    \n",
    "    gleif_file = s3_source.Object(os.environ['S3_LANDING_BUCKET'],f'mtiemann-GLEIF/ghgp-{year}-matches.csv')\n",
    "    gleif_file.download_file(f'/tmp/ghgp-gleif.csv')\n",
    "    gleif_df = pd.read_csv(f'/tmp/ghgp-gleif.csv', header=0, sep=',', dtype=str, engine='c')\n",
    "    gleif_df.dropna(subset=['LEI'], inplace=True)\n",
    "    gleif_dict = dict(zip(gleif_df['PARENT COMPANY NAME'], gleif_df.LEI))\n",
    "    \n",
    "    df['LEI'] = df['PARENT COMPANY NAME'].map(gleif_dict)\n",
    "    print(f'year = {year}; non-NULL LEIs = {len(df[df.LEI.notnull()])}')\n",
    "    \n",
    "    cols = df.columns.tolist()\n",
    "    ghgp_parent[year] = df[cols[0:2] + [cols[-1]] + cols[2:-1]]\n",
    "    osc.enforce_sql_column_names(ghgp_parent[year], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80970c5d-8194-418d-9965-cf44325c3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a next step, load per-year data, which is more detailed\n",
    "# i.e., bObj = bucket.Object(f'EPA/ghgp_2020_data_summary_spreadsheets/ghgp_data_{year}.xlsx')\n",
    "\n",
    "bObj = bucket.Object(f'EPA/ghgp_2020_data_summary_spreadsheets/ghgp_data_by_year.xlsx')\n",
    "ghgp_bytes = io.BytesIO(bObj.get()['Body'].read())\n",
    "timestamp = max(timestamp, bObj.last_modified.isoformat())\n",
    "    \n",
    "ghg_data = pd.read_excel(ghgp_bytes, sheet_name=None, dtype={'FRS Id':'string'}, skiprows=3, engine='openpyxl')\n",
    "for df_name, df in ghg_data.items():\n",
    "    if df_name=='Industry Type':\n",
    "        break\n",
    "    if 'Zip Code' in df:\n",
    "        df['Zip Code'] = df['Zip Code'].astype(str).str.zfill(5)\n",
    "    elif 'Reported Zip Code' in df:\n",
    "        df['Reported Zip Code'] = df['Reported Zip Code'].astype(str).str.zfill(5)\n",
    "    df['Primary NAICS Code'] = df['Primary NAICS Code'].astype(str)\n",
    "    df.replace('confidential', -1, inplace=True)\n",
    "    if df_name=='CO2 Injection':\n",
    "        # All info essentially confidential, but at least we know where facilities are\n",
    "        ghg_data[df_name] = df\n",
    "        continue\n",
    "    df = df.convert_dtypes()\n",
    "    # Some numeric info marked 'confidential' which will make for interesting NA handling...\n",
    "    # df.info(verbose=True)\n",
    "    ghg_data[df_name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cfcd49-90cc-45c7-a4e0-8b81947d8d07",
   "metadata": {},
   "source": [
    "Construct the combined metadata by merging existing table metadata and custom metadata.\n",
    "Note: The metadata content must be JSON serialisable and encoded as bytes; the metadata key must also be encoded as bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34490f08-dc88-43e5-87fa-7415013a1ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename = 'parent_company'\n",
    "df = pd.concat(ghgp_parent, ignore_index=True).convert_dtypes()\n",
    "osc.enforce_sql_column_names(df, inplace=True)\n",
    "drop_table = engine.execute(f\"drop table if exists {ingest_schema}.{tablename}\")\n",
    "drop_table.fetchall()\n",
    "\n",
    "columnschema = osc.create_table_schema_pairs(df, typemap={\"datetime64[ns]\":\"timestamp(6)\"})\n",
    "tabledef = f\"\"\"\n",
    "create table {tablename} (\n",
    "{columnschema}\n",
    ") with (\n",
    "format = 'ORC',\n",
    "partitioning = array['reporting_year']\n",
    ")\n",
    "\"\"\"\n",
    "print(tabledef)\n",
    "qres = engine.execute(tabledef)\n",
    "print(qres.fetchall())\n",
    "df.to_sql(tablename,\n",
    "          con=engine, schema=ingest_schema, if_exists='append',\n",
    "          index=False,\n",
    "          method=osc.TrinoBatchInsert(batch_size = 10000, verbose = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056a4a15-2fb6-4ed7-8df9-5bcc9a401133",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Melt the generation data into a more tidy format, dropping NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbee6c2-4000-495e-916e-6d16dee6d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tablename, ghg_table in ghg_data.items():\n",
    "    if tablename=='Industry Type':\n",
    "        break\n",
    "    print(f':{tablename}:')\n",
    "    # Melt the data...\n",
    "    if tablename in ['Gathering & Boosting', 'Transmission Pipelines', 'Geologic Sequestration of CO2']:\n",
    "        ghg_value_vars = ghg_table.columns[-5:]\n",
    "        ghg_id_vars = ghg_table.columns[:-5]\n",
    "    elif tablename=='Suppliers':\n",
    "        continue\n",
    "    else:\n",
    "        ghg_value_vars = ghg_table.columns[-10:]\n",
    "        ghg_id_vars = ghg_table.columns[:-10]\n",
    "    \n",
    "    # We leave in place the fact that all total reported emissions retain their categorization as to source\n",
    "    # It's temping to rename these all to 'total_reported_emissions' so that they'd magically sum together if asked.\n",
    "    # But there's no easy way in SQL to do that join without the tables exploding (because there's no natural key)\n",
    "    _, value_name = ghg_value_vars[0].split(' ', 1)\n",
    "    # value_name = 'total_reported_emissions'\n",
    "    \n",
    "    ghg_melted_df = ghg_table.melt(ghg_id_vars, ghg_value_vars, var_name='year', value_name=value_name)\n",
    "    ghg_melted_df.dropna(subset=[value_name],inplace=True)\n",
    "    ghg_melted_df.year = ghg_melted_df.year.apply(lambda x: x.split(' ', 1)[0])\n",
    "    ghg_melted_df['year'] = pd.to_datetime(ghg_melted_df['year'], format='%Y', utc=True)\n",
    "    ghg_melted_df[value_name] = ghg_melted_df[value_name].astype('float64')\n",
    "    ghg_melted_df = ghg_melted_df.convert_dtypes()\n",
    "    # Put year at the end to make for more friendly partitioning\n",
    "    ghg_melted_df = ghg_melted_df[ghg_melted_df.columns[:-2].to_list() + [ghg_melted_df.columns[-1], ghg_melted_df.columns[-2]]]\n",
    "    tablename = osc.sql_compliant_name(tablename)\n",
    "    osc.enforce_sql_column_names(ghg_melted_df, inplace=True)\n",
    "    drop_table = engine.execute(f\"drop table if exists {ingest_schema}.{tablename}\")\n",
    "    drop_table.fetchall()\n",
    "\n",
    "    columnschema = osc.create_table_schema_pairs(ghg_melted_df, typemap={\"datetime64[ns, UTC]\":\"timestamp(6)\"})\n",
    "    tabledef = f\"\"\"\n",
    "create table {tablename} (\n",
    "{columnschema}\n",
    ") with (\n",
    "    format = 'ORC',\n",
    "    partitioning = array['year']\n",
    ")\n",
    "\"\"\"\n",
    "    print(tabledef)\n",
    "    qres = engine.execute(tabledef)\n",
    "    print(qres.fetchall())\n",
    "    ghg_melted_df.to_sql(tablename,\n",
    "                         con=engine, schema=ingest_schema, if_exists='append',\n",
    "                         index=False,\n",
    "                         method=osc.TrinoBatchInsert(batch_size = 10000, verbose = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03d226a-84aa-4672-b443-b73dde9ca7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename = 'Suppliers'\n",
    "ghg_table = ghg_data[tablename].copy()\n",
    "ghg_table['Zip Code'] = ghg_table['Zip Code'].astype(str).str.zfill(5)\n",
    "\n",
    "ghg_value_names = [':'.join(list(reversed(x.split(' ', 1)))) for x in ghg_table.columns[-70:]]\n",
    "ghg_table = ghg_table.rename(columns=dict(zip(ghg_table.columns[-70:], ghg_value_names)))\n",
    "ghg_id_vars = ghg_table.columns[:-70]\n",
    "# print(ghg_id_vars)\n",
    "# print(ghg_value_names)\n",
    "stubnames = [x.split(':', 1)[0] for x in ghg_value_names[0:70:10]]\n",
    "# print(stubnames)\n",
    "suppliers_df = pd.wide_to_long(ghg_table, stubnames=stubnames, i=ghg_id_vars, j='year', sep=':')\n",
    "# Take care to null out all the `confidential` data\n",
    "for sn in stubnames:\n",
    "    suppliers_df[sn] = suppliers_df[sn].astype('Float64')\n",
    "suppliers_df.dropna(subset=stubnames, how='all', inplace=True)\n",
    "suppliers_df.reset_index(inplace=True)\n",
    "suppliers_df.loc[suppliers_df.year.notnull(), 'year'] = pd.to_datetime(suppliers_df.year, format='%Y', utc=True)\n",
    "suppliers_df = suppliers_df.convert_dtypes()\n",
    "# Put year at the end to make for more friendly partitioning\n",
    "year_index = suppliers_df.columns.get_loc('year')\n",
    "suppliers_df = suppliers_df[suppliers_df.columns[:year_index].to_list()\n",
    "                            + suppliers_df.columns[year_index+1:].to_list()\n",
    "                            + [suppliers_df.columns[year_index]]]\n",
    "\n",
    "for sn in stubnames:\n",
    "    new_stubname = ' '.join(sn.split(' ')[4:] + ['ghg'])\n",
    "    suppliers_df.rename(columns={sn:new_stubname}, inplace=True)\n",
    "\n",
    "tablename = osc.sql_compliant_name(tablename)\n",
    "osc.enforce_sql_column_names(suppliers_df, inplace=True)\n",
    "drop_table = engine.execute(f\"drop table if exists {ingest_schema}.{tablename}\")\n",
    "drop_table.fetchall()\n",
    "\n",
    "columnschema = osc.create_table_schema_pairs(suppliers_df, typemap={\"datetime64[ns, UTC]\":\"timestamp(6)\"})\n",
    "tabledef = f\"\"\"\n",
    "create table {tablename} (\n",
    "{columnschema}\n",
    ") with (\n",
    "format = 'ORC',\n",
    "partitioning = array['year']\n",
    ")\n",
    "\"\"\"\n",
    "print(tabledef)\n",
    "qres = engine.execute(tabledef)\n",
    "print(qres.fetchall())\n",
    "suppliers_df.to_sql(tablename,\n",
    "                    con=engine, schema=ingest_schema, if_exists='append',\n",
    "                    index=False,\n",
    "                    method=osc.TrinoBatchInsert(batch_size = 10000, verbose = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f5b1e6-17a4-4401-a82b-c998feee4efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything below here is speculative / in process of design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fb2656-b4e4-4693-a6a0-b76018282398",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load metadata following an ingestion process into trino metadata store\n",
    "\n",
    "### The schema is *metastore*, and the table names are *meta_schema*, *meta_table*, *meta_field*"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7976a634-6f10-41a1-a319-a71af58e8ded",
   "metadata": {},
   "source": [
    "# Create metastore structure\n",
    "metastore = {'catalog':'osc_datacommons_dev',\n",
    "             'schema':'epa_ghgrp_md',\n",
    "             'table':tablename,\n",
    "             'metadata':custom_meta_content,\n",
    "             'uuid':ingest_uuid}\n",
    "# Create DataFrame\n",
    "df_meta = pd.DataFrame(metastore)\n",
    "# Print the output\n",
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ce772-2b2e-45a1-8256-c436ab385fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
